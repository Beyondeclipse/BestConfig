Size.spark.memory.offHeap.size: "[0,16777216]"
spark.shuffle.memoryFraction: "[0,1]"
spark.storage.unrollFraction: "[0,1]"
spark.memory.storageFraction: "[0,1]"
spark.memory.fraction: "[0,1]"
spark.storage.memoryFraction: "[0,1]"
spark.speculation.quantile: "[0,1]"
spark.task.cpus: "[1,2]"
spark.rpc.message.maxSize: "[1,1024]"
spark.shuffle.sort.bypassMergeThreshold: "[1,500]"
spark.shuffle.service.index.cache.entries: "[1,10240]"
spark.scheduler.minRegisteredResourcesRatio: "[0,1]"
spark.executor.cores: "[1,16]"
Time.spark.speculation.interval: "[10,10000]"
Time.spark.executor.heartbeatInterval: "[1000,30000]"
Time.spark.network.timeout: "[30,360]"
Time.spark.rpc.lookupTimeout: "[30,360]"
spark.files.maxPartitionBytes: "[16777216,1342177280]"
Size.spark.driver.memory: "[1048576,16777216]"
Size.spark.executor.memory: "[1048576,16777216]"
Time.spark.scheduler.revive.interval: "[1000,10000]"
Time.spark.dynamicAllocation.schedulerBacklogTimeout: "[1000,10000]"
Size.spark.storage.memoryMapThreshold: "[1024,1048576]"
Size.spark.shuffle.file.buffer: "[16,1048576]"
Time.spark.rpc.retry.wait: "[1000,30000]"
Time.spark.locality.wait: "[1000,30000]"
Size.spark.reducer.maxSizeInFlight: "[10240,1048576]"
Size.spark.broadcast.blockSize: "[1024,1048576]"
Time.spark.shuffle.io.retryWait: "[1000,: 30000]"
Time.spark.files.fetchTimeout: "[30000,600000]"
Time.spark.dynamicAllocation.executorIdleTimeout: "[30000,600000]"